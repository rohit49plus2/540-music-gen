{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from music21 import instrument, note, chord, stream\n",
    "from PIL import Image\n",
    "from numpy.random import rand, normal, randn, randint\n",
    "from numpy import zeros, vstack, asarray, expand_dims, ones\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Reshape, BatchNormalization\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU, ReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training X: (866, 106, 106)\n",
      "Size of Testing X: (348, 106, 106)\n",
      "Size of Training Y: (866,)\n",
      "Size of Testing Y: (348,)\n"
     ]
    }
   ],
   "source": [
    "filelist = glob.glob(r'C:\\Users\\Harshinee Sriram\\OneDrive\\Desktop\\UBC STUDY\\CPSC 540\\Project\\DATASETS\\Working\\Train\\music\\*.png')\n",
    "trainX = np.array([np.array(Image.open(this_img)) for this_img in filelist])\n",
    "print(\"Size of Training X:\", trainX.shape)\n",
    "filelist = glob.glob(r'C:\\Users\\Harshinee Sriram\\OneDrive\\Desktop\\UBC STUDY\\CPSC 540\\Project\\DATASETS\\Working\\Validation\\music\\*.png')\n",
    "testX = np.array([np.array(Image.open(this_img)) for this_img in filelist])\n",
    "print(\"Size of Testing X:\", testX.shape)\n",
    "trainY = []\n",
    "for i in range(866):\n",
    "    trainY.append('1')\n",
    "    \n",
    "trainY = np.array(trainY)\n",
    "print(\"Size of Training Y:\", trainY.shape)\n",
    "\n",
    "testY = []\n",
    "for i in range(348):\n",
    "    testY.append('1')\n",
    "    \n",
    "testY = np.array(testY)\n",
    "print(\"Size of Testing Y:\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Harshinee Sriram\\OneDrive\\Desktop\\UBC STUDY\\CPSC 540\\Project\\DATASETS\\Working\\Train\\music'\n",
    "os.getcwd()\n",
    "img_list = os.listdir(path)\n",
    "def access_images(img_list,path,length):\n",
    "    pixels = []\n",
    "    imgs = []\n",
    "    for i in range(length):\n",
    "        if 'png' in img_list[i]:\n",
    "            try:\n",
    "                img = Image.open(path+'/'+img_list[i],'r')\n",
    "                img = img.convert('1')\n",
    "                pix = np.array(img.getdata())\n",
    "                pix = pix.astype('float32')\n",
    "                pix /= 255.0\n",
    "                pixels.append(pix.reshape(106,106,1))\n",
    "                imgs.append(img)\n",
    "            except:\n",
    "                pass\n",
    "    return np.array(pixels),imgs\n",
    "def show_image(pix_list):\n",
    "    array = np.array(pix_list.reshape(106,106), dtype=np.uint8)\n",
    "    new_image = Image.fromarray(array)\n",
    "    new_image.show()\n",
    "    \n",
    "pixels,imgs = access_images(img_list,path,200)\n",
    "\n",
    "def train_preprocessing():\n",
    "    filelist = glob.glob(r'C:\\Users\\Harshinee Sriram\\OneDrive\\Desktop\\UBC STUDY\\CPSC 540\\Project\\DATASETS\\Working\\Train\\music\\*.png')\n",
    "    trainX = np.array([np.array(Image.open(this_img)) for this_img in filelist])\n",
    "    trainX = expand_dims(trainX, axis=-1)\n",
    "    trainX = trainX.astype('float32')\n",
    "    trainX = trainX/255.0\n",
    "    return trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(in_shape = (106,106,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    n_nodes = 128 * 53 * 53\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((53, 53, 128)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Conv2DTranspose(512, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Conv2D(1, (7,7) , padding='same',activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    d_model.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict(x_input)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def performance(epoch, generator, discriminator, dataset, n_samples=50):\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    _, acc_real = discriminator.evaluate(X_real, y_real, verbose=0)\n",
    "    x_fake, y_fake = generate_fake_samples(generator, 100, n_samples)\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=50, n_batch=10):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            #print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        if (i+1) % 10 == 0:\n",
    "            performance(i, g_model, d_model, dataset)\n",
    "            #clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 106, 106, 1)\n",
      ">Accuracy real: 100%, fake: 0%\n",
      ">Accuracy real: 100%, fake: 34%\n",
      ">Accuracy real: 64%, fake: 94%\n",
      ">Accuracy real: 80%, fake: 100%\n",
      ">Accuracy real: 94%, fake: 100%\n",
      ">Accuracy real: 88%, fake: 100%\n",
      ">Accuracy real: 90%, fake: 96%\n",
      ">Accuracy real: 98%, fake: 100%\n",
      ">Accuracy real: 84%, fake: 100%\n",
      ">Accuracy real: 96%, fake: 100%\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator(latent_dim)\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "print(pixels.shape)\n",
    "train(g_model, d_model, gan_model, np.array(pixels), latent_dim, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Harshinee Sriram\\OneDrive\\Desktop\\UBC STUDY\\CPSC 540\\Project\\Jupyter Notebooks\\Output\\Sophisticated Model\\100 Epochs')\n",
    "model = g_model\n",
    "for i in range(5):\n",
    "    latent_points = generate_latent_points(100, 1)\n",
    "    X = g_model.predict(latent_points)\n",
    "    array = np.array(X.reshape(106,106),dtype = np.uint8)\n",
    "    array*= 255\n",
    "    new_image = Image.fromarray(array,'L')\n",
    "    new_image = new_image.save('composition_' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11236,)\n",
      "(11236,)\n",
      "(11236,)\n",
      "(11236,)\n",
      "(11236,)\n"
     ]
    }
   ],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "lowerBoundNote = 21\n",
    "def column2notes(column):\n",
    "    notes = []\n",
    "    for i in range(len(column)):\n",
    "        if column[i] > 255/2:\n",
    "            notes.append(i+lowerBoundNote)\n",
    "    return notes\n",
    "\n",
    "resolution = 0.25\n",
    "def updateNotes(newNotes,prevNotes): \n",
    "    res = {} \n",
    "    for note in newNotes:\n",
    "        if note in prevNotes:\n",
    "            res[note] = prevNotes[note] + resolution\n",
    "        else:\n",
    "            res[note] = resolution\n",
    "    return res\n",
    "\n",
    "def image2midi(image_path):\n",
    "    with Image.open(image_path) as image:\n",
    "        im_arr = np.frombuffer(image.tobytes(), dtype=np.uint8)\n",
    "        print(im_arr.shape)\n",
    "        try:\n",
    "            im_arr = im_arr.reshape((106, 106))\n",
    "        except:\n",
    "            im_arr = im_arr.reshape((106, 106,1))\n",
    "            im_arr = np.dot(im_arr, [0.33, 0.33, 0.33])\n",
    "\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    prev_notes = updateNotes(im_arr.T[0,:],{})\n",
    "    for column in im_arr.T[1:,:]:\n",
    "        notes = column2notes(column)\n",
    "        # pattern is a chord\n",
    "        notes_in_chord = notes\n",
    "        old_notes = prev_notes.keys()\n",
    "        for old_note in old_notes:\n",
    "            if not old_note in notes_in_chord:\n",
    "                new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                if offset - prev_notes[old_note] >= 0:\n",
    "                    new_note.offset = offset - prev_notes[old_note]\n",
    "                    output_notes.append(new_note)\n",
    "                elif offset == 0:\n",
    "                    new_note.offset = offset\n",
    "                    output_notes.append(new_note)                    \n",
    "                else:\n",
    "                    print(offset,prev_notes[old_note],old_note)\n",
    "\n",
    "        prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "        offset += resolution\n",
    "\n",
    "    for old_note in prev_notes.keys():\n",
    "        new_note = note.Note(old_note,quarterLength=prev_notes[old_note])\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        new_note.offset = offset - prev_notes[old_note]\n",
    "\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    prev_notes = updateNotes(notes_in_chord,prev_notes)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp=image_path.split(\"/\")[-1].replace(\".png\",\".mid\"))\n",
    "\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    image_path = os.getcwd() + '\\\\' + file\n",
    "    image2midi(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
